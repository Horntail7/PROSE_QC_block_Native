Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 70
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	7	aggregate_data_summary
	70	align
	1	all
	70	create_blocks
	70	merge_fastq
	218
Resources before job selection: {'_cores': 70, '_nodes': 9223372036854775807}
Ready jobs (77):
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
Selected jobs (70):
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	aggregate_data_summary
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
	create_blocks
Resources after job selection: {'_cores': 0, '_nodes': 9223372036854775737}

[Tue Apr 22 12:38:10 2025]
rule create_blocks:
    output: results/Chip2/barcode29/0/fastq_files.lst
    jobid: 31
    wildcards: chip=Chip2, native=barcode29, block=0


        realpath /data/data-transfer_KCRB/2025-04-15_HT27-2025-04-11_Run1/Chip2/*/fastq_pass/barcode29/*.fastq.gz             | head -n 100 > results/Chip2/barcode29/0/fastq_files.lst.temp

        nfiles=$(wc -l < results/Chip2/barcode29/0/fastq_files.lst.temp)
        nfiles_per_block=$((nfiles / 10))
        nhead=$((nfiles_per_block * (0+1)))

        head -n $nhead results/Chip2/barcode29/0/fastq_files.lst.temp | tail -n $nfiles_per_block > results/Chip2/barcode29/0/fastq_files.lst
        rm -f results/Chip2/barcode29/0/fastq_files.lst.temp
        

[Tue Apr 22 12:38:10 2025]
rule create_blocks:
    output: results/Chip2/barcode29/2/fastq_files.lst
    jobid: 33
    wildcards: chip=Chip2, native=barcode29, block=2


        realpath /data/data-transfer_KCRB/2025-04-15_HT27-2025-04-11_Run1/Chip2/*/fastq_pass/barcode29/*.fastq.gz             | head -n 100 > results/Chip2/barcode29/2/fastq_files.lst.temp

        nfiles=$(wc -l < results/Chip2/barcode29/2/fastq_files.lst.temp)
        nfiles_per_block=$((nfiles / 10))
        nhead=$((nfiles_per_block * (2+1)))

        head -n $nhead results/Chip2/barcode29/2/fastq_files.lst.temp | tail -n $nfiles_per_block > results/Chip2/barcode29/2/fastq_files.lst
        rm -f results/Chip2/barcode29/2/fastq_files.lst.temp
        

[Tue Apr 22 12:38:10 2025]
rule aggregate_data_summary:
    output: AGGRE/Chip1/barcode25/non_sequential_summary.csv, AGGRE/Chip1/barcode25/sequential_summary.csv
    jobid: 213
    wildcards: chip=Chip1, native=barcode25

Full Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/snakemake/jobs.py", line 371, in shellcmd
    self.format_wildcards(self.rule.shellcmd)
  File "/usr/lib/python3/dist-packages/snakemake/jobs.py", line 822, in format_wildcards
    return format(string, **_variables)
  File "/usr/lib/python3/dist-packages/snakemake/utils.py", line 405, in format
    return fmt.format(_pattern, *args, **variables)
  File "/usr/lib/python3.8/string.py", line 163, in format
    return self.vformat(format_string, args, kwargs)
  File "/usr/lib/python3.8/string.py", line 167, in vformat
    result, _ = self._vformat(format_string, args, kwargs, used_args, 2)
  File "/usr/lib/python3.8/string.py", line 207, in _vformat
    obj, arg_used = self.get_field(field_name, args, kwargs)
  File "/usr/lib/python3.8/string.py", line 278, in get_field
    obj = getattr(obj, i)
AttributeError: 'InputFiles' object has no attribute 'sequential_summary'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/snakemake/__init__.py", line 561, in snakemake
    success = workflow.execute(
  File "/usr/lib/python3/dist-packages/snakemake/workflow.py", line 850, in execute
    success = scheduler.schedule()
  File "/usr/lib/python3/dist-packages/snakemake/scheduler.py", line 374, in schedule
    self.run(job)
  File "/usr/lib/python3/dist-packages/snakemake/scheduler.py", line 389, in run
    self.get_executor(job).run(
  File "/usr/lib/python3/dist-packages/snakemake/executors.py", line 356, in run
    super()._run(job)
  File "/usr/lib/python3/dist-packages/snakemake/executors.py", line 174, in _run
    super()._run(job)
  File "/usr/lib/python3/dist-packages/snakemake/executors.py", line 106, in _run
    self.printjob(job)
  File "/usr/lib/python3/dist-packages/snakemake/executors.py", line 112, in printjob
    job.log_info(skip_dynamic=True)
  File "/usr/lib/python3/dist-packages/snakemake/jobs.py", line 914, in log_info
    logger.shellcmd(self.shellcmd, indent=indent)
  File "/usr/lib/python3/dist-packages/snakemake/jobs.py", line 376, in shellcmd
    raise RuleException(str(ex), rule=self.rule)
snakemake.exceptions.RuleException: 'InputFiles' object has no attribute 'sequential_summary'

RuleException in line 171 of /data/Analysis/2025-04-15_HT27-2025-04-11_Run1/snakefile:
'InputFiles' object has no attribute 'sequential_summary'
unlocking
removing lock
removing lock
removed all locks
